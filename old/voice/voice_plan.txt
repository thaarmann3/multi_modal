VOICE PROCESSING PIPELINE (PROCESS 2)
Offline Speech → NN / Control Loop Interface
UR5 + Python + 100 Hz speedL

==================================================

GOAL
Convert microphone audio into clean, low-rate natural-language events that
can safely influence a neural net and downstream potential-field control,
without disturbing a 100 Hz velocity control loop.

Speech is treated as a semantic sensor, not a control input.

==================================================
WHAT PROCESS 2 PUBLISHES
==================================================

Speech is published as events, not commands.

----------------------------------
1. Core Speech Event Payload
----------------------------------
Each finalized utterance produces one event containing:

- seq_id
  Monotonically increasing integer

- session_id (optional)
  Useful if conversational context is needed

- t_capture_start
  Timestamp of speech start

- t_capture_end
  Timestamp of speech end

- utterance_ms
  Duration of utterance

- transcript_raw
  Raw ASR output

- transcript_normalized
  Cleaned and normalized text fed to NN

- is_final
  Boolean (true for committed utterances)

- language (optional)

----------------------------------
2. Quality and Gating Metadata
----------------------------------
Used downstream to decide whether the NN should consume the event:

- avg_logprob (or equivalent ASR score)
- no_speech_prob (if available)
- vad_speech_ratio
  Fraction of frames classified as speech
- snr_estimate (optional)
- dropped_frames (if any)

----------------------------------
3. Safety Events (Fast Lane)
----------------------------------
Published independently of normal speech events:

- StopLatch
  Fields:
    - timestamp
    - trigger_source (keyword spotter or ASR partial)

StopLatch is latched downstream and immediately suppresses motion.

==================================================
PIPELINE STAGES
==================================================

----------------------------------
STAGE A: AUDIO CAPTURE
----------------------------------

Responsibilities:
- Continuous microphone capture
- Fixed-size audio frames (20–30 ms)
- Push frames into a ring buffer

Constraints:
- Must not block
- Must not perform ML or allocation-heavy work
- Must return immediately

Output:
- Timestamped audio frames

----------------------------------
STAGE B: VAD + UTTERANCE SEGMENTATION
----------------------------------

Responsibilities:
- Classify frames as speech or non-speech
- Assemble frames into utterances

Key parameters:
- Frame size: 20–30 ms
- Start debounce: 60–150 ms of speech
- End hangover: 300–800 ms of silence
- Pre-roll buffer: 200–500 ms
- Max utterance length: 8–12 s

Outputs:
- UtteranceAudio objects containing:
  - PCM audio
  - t_start, t_end
  - VAD statistics

Purpose:
- Gate ASR compute
- Prevent CPU spikes
- Improve transcription quality

----------------------------------
STAGE C: ASR DECODE (OFFLINE)
----------------------------------

Responsibilities:
- Convert utterance audio into text
- Produce decoding quality metrics

Design:
- Offline ASR only
- One decode at a time
- Prefer running in its own OS process
- No streaming required initially

Outputs:
- TranscriptResult:
  - text
  - timing
  - quality metrics

----------------------------------
STAGE D: TEXT NORMALIZATION
----------------------------------

Responsibilities:
- Normalize casing
- Normalize numbers ("point two" → "0.2")
- Remove transcription artifacts cautiously
- Standardize phrasing if helpful for NN input distribution

Outputs:
- transcript_normalized
- transcript_raw preserved for debugging

Rationale:
- Neural nets benefit from consistent input distributions

----------------------------------
STAGE E: EVENT PUBLICATION
----------------------------------

Two independent streams:

1. Speech Event Stream
   - Finalized utterance events
   - Delivered asynchronously

2. Safety Event Stream
   - StopLatch events
   - High priority

Transport:
- Non-blocking publish (e.g., ZeroMQ PUB/SUB)
- Messages are small and self-contained

==================================================
GATING RULES BEFORE PUBLISHING TO NN
==================================================

Publish speech events to NN only if:
- Utterance duration is within bounds
- VAD speech ratio exceeds threshold
- no_speech_prob is low (if available)
- ASR quality metric exceeds threshold

Otherwise:
- Suppress event or publish diagnostic-only event

==================================================
LATENCY EXPECTATIONS
==================================================

- End-of-speech hangover: 300–800 ms
- ASR decode time: model dependent
- Speech events update parameters/state only

The 100 Hz speedL control loop remains unaffected.

==================================================
DESIGN PRINCIPLES
==================================================

- Speech never directly modulates velocity
- Speech updates are applied via state/parameter changes
- Safety events preempt all other logic
- Heavy compute is isolated from control
- All interfaces are non-blocking

==================================================
MINIMAL EVENT TYPES
==================================================

1. SpeechUtteranceFinal
2. StopLatch

Optional (future):
3. SpeechUtterancePartial
4. SpeechRejected

==================================================
END
==================================================
